title: 基于Node实现解析fiddle的saz文件
date: 2016-08-16 21:09:37
tags:
- Web
- Node
---

### 说明
+ node-saz-parser-with-gunzip是参考saz-parser实现的，saz-parser的实现策略是使用unzip进行解压后针对每一个实体进行相应的获取报文里面的键值对，但是如果相应的数据是gzip压缩过的话，也是直接获取的gzip压缩数据。
+ node-saz-parser-with-gunzip主要是针对需要实现gzip压缩数据解压而做的，处理的策略主要是先使用unzip解压出所有的文件，然后针对每个文件来进行解析，同时针对gunzip数据进行特殊处理，处理策略下面会说明

### 预备知识
+ 字符编码
+ http报文头里面的Content-Encoding和里面的TransferEncoding,相应的知识[参考](https://imququ.com/post/transfer-encoding-header-in-http.html);

### 实现说明
> 这里主要针对解析gunzip数据进行说明，其他的解析出每个请求与响应的键值对主要是利用每个文件里面的HTTP协议的规范；主要是报文头与内容之间相隔两个回车换行，与每个键值对之间用`: `相隔等

+ 首先要注意每个saz文件的组织形式，其主要是一个zip文件，可以使用zip解压出来看看，可以看到其组织格式为一个raw文件夹和_index.html，请求与响应在raw里面，查看raw里面知道每一个请求与响应为单独的一个文件；
+ 针对内容编码为gzip，传输编码为chunked的文件的组织形式为，前面为报文头，后面为编码内容，中间用两个回车换行隔开了；
+ 对于编码内容的组织为，可以看成是几个chunk组成，每个chunk由一个占一行的长度数值和下面的内容组织，占一行的长度数值大小不包括本行的回车换行字节数，也不包括内容的回车换行字节数，所以在计算的时候这里要注意，也就是使用read的时候，其数据偏移应该是上次的内容长度加上一个回车换行的字节数，加上本次的长度数值所占的字节数加上一个回车换行的字节数；
+ 所以总的解析策略就是
    - 首先获取一次全部的content数据，然后通过`\r\n`进行分割得到一个数组，因为chunked的组织形式我们可以知道在这个数组里面有全部的长度数组，其形式是一个十六进制的数据
    - 对得到的数组进行判断过滤得到整个的chunk长度的数组
    - 针对每个chunk数组的值对文件的内容进行读取，读取的node API是fs里面的readSync，因为我们需要的是二进制的数据同时还需要进行局部读取，所以要使用readSync; 每次读取的时候其文件指针的偏移就是报文头加上两个回车换行(报文头与内容的隔离是两个回车换行)加上之前已经读取的内容长度(这里还要对应加上一个回车换行，因为内容长度是不包括最后的回车换行的)和对应的chunk长度字符的字节数加上一个回国换行；
+ 源码 [github](https://github.com/sysuKinthon/node-saz-parser-with-gunzip)